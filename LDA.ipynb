{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Food Group based on Nutrition Dataset (Classification Problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nutrition Dataset contains 8618 rows and 39 columns\n",
    "\n",
    "LDA analysis for Feature Reduction\n",
    "\n",
    "Logistic Model for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy_kcal</th>\n",
       "      <th>Protein_g</th>\n",
       "      <th>Fat_g</th>\n",
       "      <th>Carb_g</th>\n",
       "      <th>Sugar_g</th>\n",
       "      <th>Fiber_g</th>\n",
       "      <th>VitA_mcg</th>\n",
       "      <th>VitB6_mg</th>\n",
       "      <th>VitB12_mcg</th>\n",
       "      <th>VitC_mg</th>\n",
       "      <th>...</th>\n",
       "      <th>Niacin_USRDA</th>\n",
       "      <th>Riboflavin_USRDA</th>\n",
       "      <th>Thiamin_USRDA</th>\n",
       "      <th>Calcium_USRDA</th>\n",
       "      <th>Copper_USRDA</th>\n",
       "      <th>Magnesium_USRDA</th>\n",
       "      <th>Phosphorus_USRDA</th>\n",
       "      <th>Selenium_USRDA</th>\n",
       "      <th>Zinc_USRDA</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.59</td>\n",
       "      <td>4.76</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.98</td>\n",
       "      <td>7.94</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>2.38</td>\n",
       "      <td>1.59</td>\n",
       "      <td>8.73</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.79</td>\n",
       "      <td>7.94</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8613</th>\n",
       "      <td>296</td>\n",
       "      <td>3.74</td>\n",
       "      <td>14.82</td>\n",
       "      <td>36.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174813</td>\n",
       "      <td>0.049231</td>\n",
       "      <td>0.089167</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.073810</td>\n",
       "      <td>0.172857</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.051818</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8614</th>\n",
       "      <td>289</td>\n",
       "      <td>3.49</td>\n",
       "      <td>14.04</td>\n",
       "      <td>37.20</td>\n",
       "      <td>0.28</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160250</td>\n",
       "      <td>0.043846</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.014167</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.177143</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.046364</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8615</th>\n",
       "      <td>255</td>\n",
       "      <td>3.26</td>\n",
       "      <td>13.18</td>\n",
       "      <td>30.87</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151062</td>\n",
       "      <td>0.043077</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.076190</td>\n",
       "      <td>0.167143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038182</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8616</th>\n",
       "      <td>282</td>\n",
       "      <td>3.41</td>\n",
       "      <td>14.13</td>\n",
       "      <td>35.20</td>\n",
       "      <td>0.85</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169813</td>\n",
       "      <td>0.036923</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.164286</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.041818</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8617</th>\n",
       "      <td>19</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.70</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.005385</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8618 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Energy_kcal  Protein_g  Fat_g  Carb_g  Sugar_g  Fiber_g  VitA_mcg  \\\n",
       "0              40       2.38   1.59    4.76     0.79      0.0         0   \n",
       "1              56       2.38   1.98    7.94     0.79      0.8         0   \n",
       "2              56       2.38   1.59    8.73     0.79      0.8         0   \n",
       "3              16       0.81   0.81    0.81     0.81      0.0         0   \n",
       "4              48       1.59   0.79    7.94     1.59      0.8         0   \n",
       "...           ...        ...    ...     ...      ...      ...       ...   \n",
       "8613          296       3.74  14.82   36.90     0.00      4.1         0   \n",
       "8614          289       3.49  14.04   37.20     0.28      3.9         0   \n",
       "8615          255       3.26  13.18   30.87     0.86      3.5         0   \n",
       "8616          282       3.41  14.13   35.20     0.85      3.5         0   \n",
       "8617           19       0.10   0.00    4.70     4.60      0.1         0   \n",
       "\n",
       "      VitB6_mg  VitB12_mcg  VitC_mg  ...  Niacin_USRDA  Riboflavin_USRDA  \\\n",
       "0        0.000         0.0      0.0  ...      0.000000          0.000000   \n",
       "1        0.000         0.0      0.0  ...      0.000000          0.000000   \n",
       "2        0.000         0.0      0.0  ...      0.000000          0.000000   \n",
       "3        0.000         0.0      0.0  ...      0.000000          0.000000   \n",
       "4        0.000         0.0      0.0  ...      0.000000          0.000000   \n",
       "...        ...         ...      ...  ...           ...               ...   \n",
       "8613     0.282         0.0      1.1  ...      0.174813          0.049231   \n",
       "8614     0.236         0.0      1.1  ...      0.160250          0.043846   \n",
       "8615     0.236         0.0      0.9  ...      0.151062          0.043077   \n",
       "8616     0.220         0.0      1.6  ...      0.169813          0.036923   \n",
       "8617     0.019         0.0     32.0  ...      0.001750          0.005385   \n",
       "\n",
       "      Thiamin_USRDA  Calcium_USRDA  Copper_USRDA  Magnesium_USRDA  \\\n",
       "0          0.000000       0.000000      0.000000         0.000000   \n",
       "1          0.000000       0.000000      0.000000         0.000000   \n",
       "2          0.000000       0.000000      0.000000         0.000000   \n",
       "3          0.000000       0.000000      0.000000         0.000000   \n",
       "4          0.000000       0.013333      0.000000         0.000000   \n",
       "...             ...            ...           ...              ...   \n",
       "8613       0.089167       0.015833      0.000128         0.073810   \n",
       "8614       0.091667       0.014167      0.000124         0.069048   \n",
       "8615       0.097500       0.015833      0.000137         0.076190   \n",
       "8616       0.100000       0.011667      0.000123         0.064286   \n",
       "8617       0.002500       0.008333      0.000008         0.007143   \n",
       "\n",
       "      Phosphorus_USRDA  Selenium_USRDA  Zinc_USRDA  Classification  \n",
       "0             0.000000        0.000000    0.000000               5  \n",
       "1             0.000000        0.000000    0.000000               5  \n",
       "2             0.000000        0.000000    0.000000               5  \n",
       "3             0.000000        0.000000    0.000000               5  \n",
       "4             0.000000        0.000000    0.000000               5  \n",
       "...                ...             ...         ...             ...  \n",
       "8613          0.172857        0.007273    0.051818              24  \n",
       "8614          0.177143        0.007273    0.046364              24  \n",
       "8615          0.167143        0.000000    0.038182              24  \n",
       "8616          0.164286        0.007273    0.041818              24  \n",
       "8617          0.001429        0.001818    0.001818              14  \n",
       "\n",
       "[8618 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nutrition = pd.read_csv('D:\\\\Data for Preprocessing\\\\Nutrition Dataset.csv')\n",
    "Nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Energy_kcal</th>\n",
       "      <th>Protein_g</th>\n",
       "      <th>Fat_g</th>\n",
       "      <th>Carb_g</th>\n",
       "      <th>Sugar_g</th>\n",
       "      <th>Fiber_g</th>\n",
       "      <th>VitA_mcg</th>\n",
       "      <th>VitB6_mg</th>\n",
       "      <th>VitB12_mcg</th>\n",
       "      <th>VitC_mg</th>\n",
       "      <th>...</th>\n",
       "      <th>Niacin_USRDA</th>\n",
       "      <th>Riboflavin_USRDA</th>\n",
       "      <th>Thiamin_USRDA</th>\n",
       "      <th>Calcium_USRDA</th>\n",
       "      <th>Copper_USRDA</th>\n",
       "      <th>Magnesium_USRDA</th>\n",
       "      <th>Phosphorus_USRDA</th>\n",
       "      <th>Selenium_USRDA</th>\n",
       "      <th>Zinc_USRDA</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.00000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "      <td>8618.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>226.438617</td>\n",
       "      <td>11.52391</td>\n",
       "      <td>10.647024</td>\n",
       "      <td>21.819062</td>\n",
       "      <td>6.560253</td>\n",
       "      <td>2.023242</td>\n",
       "      <td>93.968786</td>\n",
       "      <td>0.264369</td>\n",
       "      <td>1.225260</td>\n",
       "      <td>7.925377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213222</td>\n",
       "      <td>0.182499</td>\n",
       "      <td>0.174881</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.077965</td>\n",
       "      <td>0.222838</td>\n",
       "      <td>0.229363</td>\n",
       "      <td>0.179111</td>\n",
       "      <td>12.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>169.388910</td>\n",
       "      <td>10.55059</td>\n",
       "      <td>15.866353</td>\n",
       "      <td>27.239000</td>\n",
       "      <td>13.602098</td>\n",
       "      <td>4.313670</td>\n",
       "      <td>779.362205</td>\n",
       "      <td>0.478614</td>\n",
       "      <td>4.319183</td>\n",
       "      <td>57.582758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302161</td>\n",
       "      <td>0.346062</td>\n",
       "      <td>0.431974</td>\n",
       "      <td>0.167803</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.133496</td>\n",
       "      <td>0.290133</td>\n",
       "      <td>0.514406</td>\n",
       "      <td>0.305292</td>\n",
       "      <td>6.176887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>2.47000</td>\n",
       "      <td>0.992500</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023875</td>\n",
       "      <td>0.035385</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.052857</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>191.000000</td>\n",
       "      <td>8.28500</td>\n",
       "      <td>5.235000</td>\n",
       "      <td>8.945000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131250</td>\n",
       "      <td>0.115385</td>\n",
       "      <td>0.064583</td>\n",
       "      <td>0.015833</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.070909</td>\n",
       "      <td>0.076818</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>336.000000</td>\n",
       "      <td>19.97750</td>\n",
       "      <td>13.900000</td>\n",
       "      <td>32.900000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>1.297500</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314328</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.185833</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.069048</td>\n",
       "      <td>0.308571</td>\n",
       "      <td>0.387273</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>902.000000</td>\n",
       "      <td>88.32000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.800000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>98.890000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.968750</td>\n",
       "      <td>13.461538</td>\n",
       "      <td>19.479167</td>\n",
       "      <td>6.136667</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>1.859524</td>\n",
       "      <td>14.168571</td>\n",
       "      <td>34.854545</td>\n",
       "      <td>8.268182</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Energy_kcal   Protein_g        Fat_g       Carb_g      Sugar_g  \\\n",
       "count  8618.000000  8618.00000  8618.000000  8618.000000  8618.000000   \n",
       "mean    226.438617    11.52391    10.647024    21.819062     6.560253   \n",
       "std     169.388910    10.55059    15.866353    27.239000    13.602098   \n",
       "min       0.000000     0.00000     0.000000     0.000000     0.000000   \n",
       "25%      93.000000     2.47000     0.992500     0.040000     0.000000   \n",
       "50%     191.000000     8.28500     5.235000     8.945000     0.370000   \n",
       "75%     336.000000    19.97750    13.900000    32.900000     5.300000   \n",
       "max     902.000000    88.32000   100.000000   100.000000    99.800000   \n",
       "\n",
       "           Fiber_g      VitA_mcg     VitB6_mg   VitB12_mcg      VitC_mg  ...  \\\n",
       "count  8618.000000   8618.000000  8618.000000  8618.000000  8618.000000  ...   \n",
       "mean      2.023242     93.968786     0.264369     1.225260     7.925377  ...   \n",
       "std       4.313670    779.362205     0.478614     4.319183    57.582758  ...   \n",
       "min       0.000000      0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.000000      0.000000     0.034250     0.000000     0.000000  ...   \n",
       "50%       0.300000      1.500000     0.120000     0.080000     0.000000  ...   \n",
       "75%       2.400000     21.000000     0.355000     1.297500     2.500000  ...   \n",
       "max      79.000000  30000.000000    12.000000    98.890000  2400.000000  ...   \n",
       "\n",
       "       Niacin_USRDA  Riboflavin_USRDA  Thiamin_USRDA  Calcium_USRDA  \\\n",
       "count   8618.000000       8618.000000    8618.000000    8618.000000   \n",
       "mean       0.213222          0.182499       0.174881       0.061176   \n",
       "std        0.302161          0.346062       0.431974       0.167803   \n",
       "min        0.000000          0.000000       0.000000       0.000000   \n",
       "25%        0.023875          0.035385       0.025000       0.007500   \n",
       "50%        0.131250          0.115385       0.064583       0.015833   \n",
       "75%        0.314328          0.200000       0.185833       0.051667   \n",
       "max        7.968750         13.461538      19.479167       6.136667   \n",
       "\n",
       "       Copper_USRDA  Magnesium_USRDA  Phosphorus_USRDA  Selenium_USRDA  \\\n",
       "count   8618.000000      8618.000000       8618.000000     8618.000000   \n",
       "mean       0.000191         0.077965          0.222838        0.229363   \n",
       "std        0.000615         0.133496          0.290133        0.514406   \n",
       "min        0.000000         0.000000          0.000000        0.000000   \n",
       "25%        0.000033         0.023810          0.052857        0.001818   \n",
       "50%        0.000088         0.047619          0.190000        0.070909   \n",
       "75%        0.000161         0.069048          0.308571        0.387273   \n",
       "max        0.016722         1.859524         14.168571       34.854545   \n",
       "\n",
       "        Zinc_USRDA  Classification  \n",
       "count  8618.000000     8618.000000  \n",
       "mean      0.179111       12.013460  \n",
       "std       0.305292        6.176887  \n",
       "min       0.000000        0.000000  \n",
       "25%       0.020909        7.000000  \n",
       "50%       0.076818       13.000000  \n",
       "75%       0.245455       17.000000  \n",
       "max       8.268182       24.000000  \n",
       "\n",
       "[8 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Central tendency values\n",
    "Nutrition.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8618 entries, 0 to 8617\n",
      "Data columns (total 39 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Energy_kcal       8618 non-null   int64  \n",
      " 1   Protein_g         8618 non-null   float64\n",
      " 2   Fat_g             8618 non-null   float64\n",
      " 3   Carb_g            8618 non-null   float64\n",
      " 4   Sugar_g           8618 non-null   float64\n",
      " 5   Fiber_g           8618 non-null   float64\n",
      " 6   VitA_mcg          8618 non-null   int64  \n",
      " 7   VitB6_mg          8618 non-null   float64\n",
      " 8   VitB12_mcg        8618 non-null   float64\n",
      " 9   VitC_mg           8618 non-null   float64\n",
      " 10  VitE_mg           8618 non-null   float64\n",
      " 11  Folate_mcg        8618 non-null   int64  \n",
      " 12  Niacin_mg         8618 non-null   float64\n",
      " 13  Riboflavin_mg     8618 non-null   float64\n",
      " 14  Thiamin_mg        8618 non-null   float64\n",
      " 15  Calcium_mg        8618 non-null   int64  \n",
      " 16  Copper_mcg        8618 non-null   float64\n",
      " 17  Iron_mg           8618 non-null   float64\n",
      " 18  Magnesium_mg      8618 non-null   int64  \n",
      " 19  Manganese_mg      8618 non-null   float64\n",
      " 20  Phosphorus_mg     8618 non-null   int64  \n",
      " 21  Selenium_mcg      8618 non-null   float64\n",
      " 22  Zinc_mg           8618 non-null   float64\n",
      " 23  VitA_USRDA        8618 non-null   float64\n",
      " 24  VitB6_USRDA       8618 non-null   float64\n",
      " 25  VitB12_USRDA      8618 non-null   float64\n",
      " 26  VitC_USRDA        8618 non-null   float64\n",
      " 27  VitE_USRDA        8618 non-null   float64\n",
      " 28  Folate_USRDA      8618 non-null   float64\n",
      " 29  Niacin_USRDA      8618 non-null   float64\n",
      " 30  Riboflavin_USRDA  8618 non-null   float64\n",
      " 31  Thiamin_USRDA     8618 non-null   float64\n",
      " 32  Calcium_USRDA     8618 non-null   float64\n",
      " 33  Copper_USRDA      8618 non-null   float64\n",
      " 34  Magnesium_USRDA   8618 non-null   float64\n",
      " 35  Phosphorus_USRDA  8618 non-null   float64\n",
      " 36  Selenium_USRDA    8618 non-null   float64\n",
      " 37  Zinc_USRDA        8618 non-null   float64\n",
      " 38  Classification    8618 non-null   int64  \n",
      "dtypes: float64(32), int64(7)\n",
      "memory usage: 2.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Analyse Column values\n",
    "Nutrition.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8618, 38) (8618,)\n"
     ]
    }
   ],
   "source": [
    "# split data into inputs and outputs\n",
    "X = Nutrition.iloc[:, :-1]\n",
    "y = Nutrition.iloc[:, -1]\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evaluate pca with logistic regression algorithm for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pipeline\n",
    "steps = [('norm', MinMaxScaler()), ('lda', LinearDiscriminantAnalysis(n_components=10)), ('m', LogisticRegression())]\n",
    "model = Pipeline(steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.644 (0.013)\n"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA transform with logistic regression achieved a performance of about 64.4 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Compare PCA number of components with logistic regression algorithm for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    X = Nutrition.iloc[:, :-1]\n",
    "    y = Nutrition.iloc[:, -1]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    " models = dict()\n",
    " for i in range(1,39):\n",
    "    steps = [('norm', MinMaxScaler()), ('lda', LinearDiscriminantAnalysis(n_components=i)), ('m', LogisticRegression())]\n",
    "    models[str(i)] = Pipeline(steps=steps)\n",
    " return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    " cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    " return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 0.298 (0.005)\n",
      ">2 0.364 (0.009)\n",
      ">3 0.411 (0.011)\n",
      ">4 0.478 (0.012)\n",
      ">5 0.533 (0.011)\n",
      ">6 0.575 (0.013)\n",
      ">7 0.588 (0.014)\n",
      ">8 0.609 (0.015)\n",
      ">9 0.630 (0.013)\n",
      ">10 0.644 (0.013)\n",
      ">11 0.647 (0.012)\n",
      ">12 0.656 (0.013)\n",
      ">13 0.666 (0.014)\n",
      ">14 0.677 (0.013)\n",
      ">15 0.683 (0.014)\n",
      ">16 0.689 (0.011)\n",
      ">17 0.691 (0.011)\n",
      ">18 0.694 (0.011)\n",
      ">19 0.699 (0.011)\n",
      ">20 0.703 (0.012)\n",
      ">21 0.706 (0.012)\n",
      ">22 0.707 (0.010)\n",
      ">23 0.709 (0.010)\n",
      ">24 0.709 (0.010)\n",
      ">25 nan (nan)\n",
      ">26 nan (nan)\n",
      ">27 nan (nan)\n",
      ">28 nan (nan)\n",
      ">29 nan (nan)\n",
      ">30 nan (nan)\n",
      ">31 nan (nan)\n",
      ">32 nan (nan)\n",
      ">33 nan (nan)\n",
      ">34 nan (nan)\n",
      ">35 nan (nan)\n",
      ">36 nan (nan)\n",
      ">37 nan (nan)\n",
      ">38 nan (nan)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    " scores = evaluate_model(model, X, y)\n",
    " results.append(scores)\n",
    " names.append(name)\n",
    " print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    " # plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.xticks(rotation=45)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Shown in above fig., Accuracy increase with increasing the number of Components upto 18, afterwards it's almost same upto 23\n",
    " \n",
    "So, we choose n =23 for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using pca with 22 Number of component logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8618, 38) (8618,)\n",
      "Predicted Class: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using pca with logistic regression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# split data into inputs and outputs\n",
    "X_new = Nutrition.iloc[:, :-1]\n",
    "y_new = Nutrition.iloc[:, -1]\n",
    "print(X_new.shape, y_new.shape)\n",
    "# define the model\n",
    "steps_new = [('norm', MinMaxScaler()), ('lda', LinearDiscriminantAnalysis(n_components=23)), ('m', LogisticRegression())]\n",
    "model_new= Pipeline(steps=steps_new)\n",
    "# fit the model on the whole dataset\n",
    "model_new.fit(X_new, y_new)\n",
    "# make a single prediction\n",
    "row = [[253.25,\t2.99375,\t13.93, 28.97,\t0.96875,\t2.8875,\t0,\t0.196375,\t0,\t5.375\t,1.20625,\t0,\t2.05575,\t0.04875,\t0.078375,\t14.5,\t0.08925,\t0.67625,\t23.75,\t0.1865,\t97.875,\t0.2125,\t0.41,\t0,\t0.115514706,\t0,\t0.059722222,\t0.080416667,\t0,\t0.128484375,\t0.0375,\t0.0653125,\t0.012083333,\t9.91628E-05,\t0.056547619,\t0.139821429,\t0.003863636,\t0.037272727]]\n",
    "yhat = model_new.predict(row)\n",
    "print('Predicted Class: %d' % yhat[0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment -\n",
    "\n",
    "Fit the Pipeline on available data and makes a prediction on new data.\n",
    "Transform uses 23 most important components from LDA transform, as we found from testing above.\n",
    "a new row of data with 20 columns is provided and is automatically transformed to 22 components \n",
    "and fed to the logistic regression model in order to predict the class label\n",
    "\n",
    "ANS - Predicted class = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
